{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This file contains all the functions and structures used for conducting experiments with the simple CNN networks\n",
    "##### All of the functions are analogous to the functions contained in the ANN_CODE notebook, with some minor changes to allow for execution with the CNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            # print(validation_loss,self.min_validation_loss,(self.min_validation_loss + self.min_delta) )\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "def train_data_mirror(X_train, y_train):\n",
    "   \n",
    "   '''This Function is used to perform mirroring data augmentation (use only for train data)'''\n",
    "\n",
    "   titles = X_train.columns\n",
    "\n",
    "   columns_titles = np.concatenate((titles[int((len(titles))/2):],titles[0:int((len(titles))/2)]))\n",
    "   \n",
    "\n",
    "   X_train_2 = X_train.reindex(columns=columns_titles) \n",
    "\n",
    "   X_train_2.columns = X_train.columns\n",
    "\n",
    "   y_train_2 = 1-y_train\n",
    "   X_train = pd.concat([X_train, X_train_2], axis=0)\n",
    "   y_train = pd.concat([y_train, y_train_2], axis=0)\n",
    "\n",
    "   return X_train, y_train\n",
    "\n",
    "\n",
    "def create_data_loaders(data, batch_size, train_mirror=False, standarize=True):\n",
    "\n",
    "\n",
    "    # Scale the data?\n",
    "    if standarize:  \n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_ = scaler.fit_transform(X)\n",
    "\n",
    "        data_ = pd.DataFrame(X_)\n",
    "        data_['winner'] = y\n",
    "\n",
    "        data = data_\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = train_test_split(data, test_size=0.15)\n",
    "    train_data, validation_data = train_test_split(train_data, test_size=0.15/0.85)\n",
    " \n",
    "    # Data augmentation flip?\n",
    "\n",
    "    if train_mirror:\n",
    "        \n",
    "        X_train, y_train = train_data_mirror(train_data.iloc[:, :-1], train_data.iloc[:, -1])\n",
    "\n",
    "    # Convert data to PyTorch tensors and create data loaders\n",
    "    train_dataset = TensorDataset(torch.from_numpy(train_data.iloc[:,:-1].values.astype(np.float32)), torch.from_numpy(train_data.iloc[:, -1].values.reshape(-1, 1).astype(np.float32)))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataset = TensorDataset(torch.from_numpy(validation_data.iloc[:,:-1].values.astype(np.float32)), torch.from_numpy(validation_data.iloc[:, -1].values.reshape(-1, 1).astype(np.float32)))\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_dataset = TensorDataset(torch.from_numpy(test_data.iloc[:, :-1].values.astype(np.float32)), torch.from_numpy(test_data.iloc[: ,-1].values.reshape(-1, 1).astype(np.float32)))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(f'Data loaded -- starting model training...')\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device, mode, epoch, print_history = True):\n",
    "    model.train()\n",
    "    # Train the model for 1 epoch\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        batch_size = inputs.size(0)\n",
    "        total += batch_size\n",
    "        try:\n",
    "            loss = criterion(outputs, labels)\n",
    "        except:\n",
    "            loss = criterion(outputs.squeeze(1), labels.squeeze(1))\n",
    "\n",
    "        \n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_size / len(loader.dataset)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    accuracy =  correct/total\n",
    "    if print_history:\n",
    "        print(f\"Epoch {epoch+1} {mode} loss: {running_loss}\")\n",
    "        print(f\"Epoch {epoch+1} {mode} accuracy: {accuracy}\")\n",
    "\n",
    "    return running_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, loader, criterion, device, mode, epoch, print_history = True):\n",
    "    model.eval()\n",
    "    # Evaluate the neural network on the testing set\n",
    "    running_loss = 0.0\n",
    "    avg_inference_time = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in loader:\n",
    "            counter += 1\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            end = time.time()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            batch_size = inputs.size(0)\n",
    "            total += batch_size\n",
    "            batch_inference_time = end-start\n",
    "            avg_inference_time += batch_inference_time\n",
    "            try:\n",
    "                loss = criterion(outputs, labels)\n",
    "            except:\n",
    "                loss = criterion(outputs.squeeze(1), labels.squeeze(1))\n",
    "            running_loss += loss.item() * batch_size / len(loader.dataset)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy =  correct / total\n",
    "        \n",
    "        if print_history:\n",
    "            print(f\"Epoch {epoch+1} {mode} loss: {running_loss}\")\n",
    "            print(f\"Epoch {epoch+1} {mode} accuracy: {accuracy}\")\n",
    "        \n",
    "    return running_loss, accuracy, avg_inference_time/counter\n",
    "\n",
    "\n",
    "def plot_loss(train_loss, validation_loss, title):\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"subsequent epochs\",fontsize=16)\n",
    "    plt.ylabel('average loss',fontsize=16)\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
    "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
    "    plt.legend()\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.tick_params(labelsize=12)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_acc(train_loss, validation_loss, title):\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"subsequent epochs\",fontsize=16)\n",
    "    plt.ylabel('average accuracy',fontsize=16)\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
    "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
    "    plt.legend()\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_CNN_model(data, model, optimizer, num_epochs, batch_size, device = 'cuda',train_mirror=False, early_stopping=False, es_patience = 5, es_delta = 0.01):\n",
    "    \n",
    "    hi_accuracy=0\n",
    "    train_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    train_acc_arr = []\n",
    "    validation_acc_arr = []\n",
    "\n",
    "    train_loader, validation_loader, test_loader = create_data_loaders(data, batch_size,train_mirror) # Initialize the dataloaders from dataset\n",
    "\n",
    "\n",
    "    # Create CNN classifier model and optimizer\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optimizer\n",
    "\n",
    "    # Train the model\n",
    "    if(early_stopping):\n",
    "        early_stopper = EarlyStopper(patience=es_patience, min_delta=es_delta) # Enable early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(model,train_loader, optimizer, criterion=criterion, device=device, mode='Train', epoch=epoch)\n",
    "        valid_loss, valid_accuracy = test(model, validation_loader, criterion=criterion, device=device, mode='Validation', epoch=epoch)\n",
    "\n",
    "        if(early_stopping):\n",
    "            if early_stopper.early_stop(valid_loss):\n",
    "                break\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        train_loss_arr.append(train_loss)\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "\n",
    "        \n",
    "\n",
    "        validation_loss_arr.append(valid_loss)\n",
    "        validation_acc_arr.append(valid_accuracy)\n",
    "\n",
    "        try:\n",
    "            if valid_accuracy>=hi_accuracy:\n",
    "                hi_accuracy = valid_accuracy\n",
    "                print(f'New best state saved with valid. acc. = {hi_accuracy}.')\n",
    "                model_state = model.state_dict()\n",
    "        except:\n",
    "            print(\"Cannot yet save model state dict.\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    plot_loss(train_loss=train_loss_arr, validation_loss=validation_loss_arr, title='CNN model loss')\n",
    "    plot_acc(train_acc_arr, validation_acc_arr, title='CNN model accuracy')\n",
    "\n",
    "    test(model, test_loader, criterion=criterion, device=device, mode='Test', epoch=epoch)\n",
    "\n",
    "    return model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader_cv(data, batch_size, train_mirror=False, standarize=True):\n",
    "\n",
    "\n",
    "    # Scale the data?\n",
    "    if standarize:  \n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_ = scaler.fit_transform(X)\n",
    "\n",
    "        data_ = pd.DataFrame(X_)\n",
    "        data_['winner'] = y\n",
    "\n",
    "        data = data_\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    # train_data, test_data = train_test_split(data, test_size=0.15)\n",
    "    # train_data, validation_data = train_test_split(train_data, test_size=0.15/0.85)\n",
    " \n",
    "    # Data augmentation flip?\n",
    "\n",
    "    if train_mirror:\n",
    "        \n",
    "        X, y = train_data_mirror(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "\n",
    "    # Convert data to PyTorch tensors and create data loaders\n",
    "    data_dataset = TensorDataset(torch.from_numpy(data.iloc[:,:-1].values.astype(np.float32)), torch.from_numpy(data.iloc[:, -1].values.reshape(-1, 1).astype(np.float32)))\n",
    "    data_loader = DataLoader(data_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(f'Data loaded succesfully!')\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_CNN_model(train_loader, test_loader, model, criterion, optimizer, num_epochs, device='cuda', early_stopping = False, es_patience = 5, es_delta = 0.01, plot_history = True, print_history = True):\n",
    "    \n",
    "    model_state = None\n",
    "    hi_accuracy = 0\n",
    "    train_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    train_acc_arr = []\n",
    "    validation_acc_arr = []\n",
    "\n",
    "\n",
    "\n",
    "    # train_loader, validation_loader, test_loader = create_data_loaders(data, batch_size,train_mirror) # Initialize the dataloaders from dataset\n",
    "\n",
    "    model = model.to(device) # Initialize linear model with layer sizes\n",
    "    # criterion = nn.BCELoss() # Initialize loss function - Binary Cross Entropy Loss\n",
    "    optimizer = optimizer # Initialize optimizer with starting learning rate\n",
    "    try:\n",
    "        criterion = criterion()\n",
    "    except:\n",
    "        print('focal loss')\n",
    "\n",
    "    # Initialize result collecting lists for later plotting\n",
    "    train_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    train_acc_arr = []\n",
    "    validation_acc_arr = []\n",
    "\n",
    "    avg_inference_time = 0\n",
    "\n",
    "    # Train the model\n",
    "    if(early_stopping):\n",
    "        early_stopper = EarlyStopper(patience=es_patience, min_delta=es_delta) # Enable early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_accuracy = train(model,train_loader, optimizer, criterion=criterion, device=device, mode='Train', epoch=epoch, print_history=print_history)\n",
    "        valid_loss, valid_accuracy, inference_time = test(model, test_loader, criterion=criterion, device=device, mode='Validation', epoch=epoch, print_history=print_history)\n",
    "        avg_inference_time += inference_time\n",
    "\n",
    "        \n",
    "        if(early_stopping):\n",
    "            if early_stopper.early_stop(valid_loss):\n",
    "                break\n",
    "\n",
    "        train_loss_arr.append(train_loss)\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "\n",
    "        try:\n",
    "            if valid_accuracy>=hi_accuracy:\n",
    "                hi_accuracy = valid_accuracy\n",
    "                print(f'New best state saved with valid. acc. = {hi_accuracy} for epoch {epoch+1}.')\n",
    "                print(f'Average inference time for a single validation batch: {inference_time}s.')\n",
    "                model_state = model.state_dict()\n",
    "        except:\n",
    "            print(\"Cannot yet save model state dict.\")\n",
    "\n",
    "        validation_loss_arr.append(valid_loss)\n",
    "        validation_acc_arr.append(valid_accuracy)\n",
    "\n",
    "\n",
    "    if plot_history:\n",
    "        plot_loss(train_loss=train_loss_arr, validation_loss=validation_loss_arr, title='CNN model loss')\n",
    "        plot_acc(train_acc_arr, validation_acc_arr, title='CNN model accuracy')\n",
    "\n",
    "    # model.load_state_dict(model_state)\n",
    "    # test(model, test_loader, criterion=criterion, device=device, mode='Test', epoch=epoch)\n",
    "\n",
    "    return hi_accuracy, avg_inference_time/num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_split_testing_CNN(data, model_class, criterion, num_of_splits, activation_f, plot_history = True, print_history = True, early_stopping = True):\n",
    "\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    splits = KFold(num_of_splits)\n",
    "\n",
    "    cv_accuracy = []\n",
    "    cv_inference_time = []\n",
    "    for i, (train_split, test_split) in enumerate(splits.split(data)):\n",
    "        print(f'Training and testing {i+1}/{num_of_splits} fold...')\n",
    "\n",
    "        train_fold = data.loc[train_split,:]\n",
    "        test_fold = data.loc[test_split,:]\n",
    "\n",
    "        train_loader = create_loader_cv(train_fold, 64, False, True)\n",
    "        test_loader = create_loader_cv(test_fold, 64, False, True)\n",
    "\n",
    "        lr = 0.001\n",
    "        model = model_class(activation_f=activation_f)\n",
    "        optimizer = optim.RAdam(model.parameters(), lr=lr)\n",
    "        \n",
    "        fold_test_accuracy, inference_time = cv_CNN_model(train_loader, test_loader, model, criterion, optimizer, num_epochs=50, early_stopping=early_stopping, es_patience=5, es_delta=0.002, plot_history=plot_history, print_history=print_history)\n",
    "\n",
    "        print(f'Highest validation accuracy in fold {i+1}/{num_of_splits}: {fold_test_accuracy}')\n",
    "\n",
    "        # try:\n",
    "        #     print(f'Avg. Inference time (a single forward pass): {model.inference_time/model.n_of_forward_passes}')\n",
    "        # except:\n",
    "        #     print('unknown inference time')\n",
    "\n",
    "        cv_accuracy.append(fold_test_accuracy)\n",
    "        cv_inference_time.append(inference_time)\n",
    "\n",
    "    print(f'Average accuracy over {num_of_splits} folds: {np.mean(cv_accuracy)}')\n",
    "    print(f'Average inference time over {num_of_splits} folds for a single validation batch: {np.mean(cv_inference_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='none'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        BCE_loss = F.binary_cross_entropy(input, target, reduction='mean')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposed CNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN model\n",
    "class CNN_team(nn.Module):\n",
    "    def __init__(self, activation_f):\n",
    "        super(CNN_team, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.bn0 = nn.BatchNorm1d(64)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.bn4 = nn.BatchNorm1d(1)\n",
    "        self.do = nn.Dropout(0.5)\n",
    "        self.activation_f = activation_f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.activation_f(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.activation_f(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = nn.AvgPool1d(kernel_size=x.shape[-1])(x)\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.bn0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_f(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.do(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation_f(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.do(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation_f(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.do(x)\n",
    "        x = torch.sigmoid(self.bn4(self.fc4(x)))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of usage for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# Load data from a CSV file\n",
    "data = pd.read_csv('Teams_statistics.csv')\n",
    "data.drop(['match_id','map','team_1','team_2'],axis=1, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "input_size = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = CNN_team\n",
    "criterion = FocalLoss(alpha=0.95, gamma=2.6)\n",
    "activation_f = nn.ReLU()\n",
    "\n",
    "\n",
    "cv_split_testing_CNN(data, model_class, num_of_splits=5, criterion=criterion, activation_f=activation_f, plot_history=False, print_history=False, early_stopping=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
