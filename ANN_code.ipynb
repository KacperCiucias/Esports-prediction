{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains all the required functions and structures to perform experiments with the ANN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "\n",
    "# Define an early stopping mechanism for regularization use - from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            # print(validation_loss,self.min_validation_loss,(self.min_validation_loss + self.min_delta) )\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "def train_data_mirror(X_train, y_train):\n",
    "   \n",
    "   '''This Function is used to perform mirroring data augmentation (use only for train data)\n",
    "        Input: X_train, y_train - dataframes of features and target\n",
    "        Output: X_train, y_train - dataframes of features and targed with mirroring applied\n",
    "   '''\n",
    "\n",
    "   titles = X_train.columns\n",
    "\n",
    "   columns_titles = np.concatenate((titles[int((len(titles))/2):],titles[0:int((len(titles))/2)]))\n",
    "   \n",
    "\n",
    "   X_train_2 = X_train.reindex(columns=columns_titles) \n",
    "\n",
    "   X_train_2.columns = X_train.columns\n",
    "\n",
    "   y_train_2 = 1-y_train\n",
    "   X_train = pd.concat([X_train, X_train_2], axis=0)\n",
    "   y_train = pd.concat([y_train, y_train_2], axis=0)\n",
    "\n",
    "   return X_train, y_train\n",
    "\n",
    "\n",
    "def create_data_loaders(data, batch_size, train_mirror=False, standarize=True):\n",
    "\n",
    "    '''Function used to create pytorch data loader objects\n",
    "        Input: \n",
    "                data - dataframe used for experiments\n",
    "                batch_size - int, number of samples to use per learning batch\n",
    "                train_mirror - bool, whether or not to add mirrored data \n",
    "                standarize - bool, whether or not to standarize the data\n",
    "        Output:\n",
    "                train_loader, valid_loader, test_loader - pytorch dataloader object used to train, validate and test the model\n",
    "    '''\n",
    "\n",
    "    # Scale the data?\n",
    "    if standarize:  \n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_ = scaler.fit_transform(X)\n",
    "\n",
    "        data_ = pd.DataFrame(X_)\n",
    "        data_['winner'] = y\n",
    "\n",
    "        data = data_\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_data, test_data = train_test_split(data, test_size=0.15)\n",
    "    train_data, validation_data = train_test_split(train_data, test_size=0.15/0.85)\n",
    " \n",
    "    # Data augmentation flip?\n",
    "\n",
    "    if train_mirror:\n",
    "        \n",
    "        X_train, y_train = train_data_mirror(train_data.iloc[:, :-1], train_data.iloc[:, -1])\n",
    "\n",
    "    # Convert data to PyTorch tensors and create data loaders\n",
    "    train_dataset = TensorDataset(torch.from_numpy(train_data.iloc[:,:-1].values.astype(np.float32)), torch.from_numpy(train_data.iloc[:, -1].values.reshape(-1, 1).astype(np.float32)))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_dataset = TensorDataset(torch.from_numpy(validation_data.iloc[:,:-1].values.astype(np.float32)), torch.from_numpy(validation_data.iloc[:, -1].values.reshape(-1, 1).astype(np.float32)))\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_dataset = TensorDataset(torch.from_numpy(test_data.iloc[:, :-1].values.astype(np.float32)), torch.from_numpy(test_data.iloc[: ,-1].values.reshape(-1, 1).astype(np.float32)))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(f'Data loaded -- starting model training...')\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device, mode, epoch, print_history = True):\n",
    "    '''\n",
    "    Function used to train the pytorch model\n",
    "    Input: \n",
    "        model - pytorch model object\n",
    "        loader - dataloader object to use for the model\n",
    "        optimizer - nn.optim object, used as a optimizer in the learning process\n",
    "        criterion - loss function\n",
    "        device - \"cpu\" or \"cuda\" depending on the available resources\n",
    "        mode - used only for prining the current mode (ex. training, validation)\n",
    "        epoch - current epoch\n",
    "        print_history - bool, whether to display the loss and accuracy for each epoch\n",
    "\n",
    "    Output: \n",
    "        running_loss - loss value up to the current epoch\n",
    "        accuracy - model accuracy caluclated in this step\n",
    "    '''\n",
    "    model.train()\n",
    "    # Train the model for 1 epoch\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        batch_size = inputs.size(0)\n",
    "        total += batch_size\n",
    "        try:\n",
    "            loss = criterion(outputs, labels)\n",
    "        except:\n",
    "            loss = criterion(outputs.squeeze(1), labels.squeeze(1))\n",
    "\n",
    "        \n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_size / len(loader.dataset)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    accuracy =  correct/total\n",
    "    if print_history:\n",
    "        print(f\"Epoch {epoch+1} {mode} loss: {running_loss}\")\n",
    "        print(f\"Epoch {epoch+1} {mode} accuracy: {accuracy}\")\n",
    "\n",
    "    return running_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, loader, criterion, device, mode, epoch, print_history = True):\n",
    "\n",
    "    '''\n",
    "       Function used to test the pytorch model\n",
    "    Input: \n",
    "        model - pytorch model object\n",
    "        loader - dataloader object to use for the model\n",
    "        optimizer - nn.optim object, used as a optimizer in the learning process\n",
    "        criterion - loss function\n",
    "        device - \"cpu\" or \"cuda\" depending on the available resources\n",
    "        mode - used only for prining the current mode (ex. training, validation)\n",
    "        epoch - current epoch\n",
    "        print_history - bool, whether to display the loss and accuracy for each epoch\n",
    "\n",
    "    Output: \n",
    "        running_loss - loss value up to the current epoch\n",
    "        accuracy - model accuracy caluclated in this step\n",
    "        inference time in [s]\n",
    "    \n",
    "    '''\n",
    "    model.eval()\n",
    "    # Evaluate the neural network on the testing set\n",
    "    running_loss = 0.0\n",
    "    avg_inference_time = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in loader:\n",
    "            counter += 1\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            start = time.time()\n",
    "            outputs = model(inputs)\n",
    "            end = time.time()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            batch_size = inputs.size(0)\n",
    "            total += batch_size\n",
    "            batch_inference_time = end-start\n",
    "            avg_inference_time += batch_inference_time\n",
    "            try:\n",
    "                loss = criterion(outputs, labels)\n",
    "            except:\n",
    "                loss = criterion(outputs.squeeze(1), labels.squeeze(1))\n",
    "            running_loss += loss.item() * batch_size / len(loader.dataset)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy =  correct / total\n",
    "        \n",
    "        if print_history:\n",
    "            print(f\"Epoch {epoch+1} {mode} loss: {running_loss}\")\n",
    "            print(f\"Epoch {epoch+1} {mode} accuracy: {accuracy}\")\n",
    "        \n",
    "    return running_loss, accuracy, avg_inference_time/counter\n",
    "\n",
    "\n",
    "# Functions used to plot accuracy and loss for the respective epochs if needed\n",
    "\n",
    "def plot_loss(train_loss, validation_loss, title):\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"subsequent epochs\",fontsize=16)\n",
    "    plt.ylabel('average loss',fontsize=16)\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
    "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
    "    plt.legend()\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.tick_params(labelsize=12)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_acc(train_loss, validation_loss, title):\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"subsequent epochs\",fontsize=16)\n",
    "    plt.ylabel('average accuracy',fontsize=16)\n",
    "    plt.plot(range(1, len(train_loss)+1), train_loss, 'o-', label='training')\n",
    "    plt.plot(range(1, len(validation_loss)+1), validation_loss, 'o-', label='validation')\n",
    "    plt.legend()\n",
    "    plt.title(title,fontsize=20)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_ANN_model(data, model, optimizer, num_epochs, batch_size, device='cuda', train_mirror=False, early_stopping = False, es_patience = 5, es_delta = 0.01):\n",
    "    \n",
    "    '''\n",
    "        Function that encapsulates the steps needed to perform a trial run of the prediction precess\n",
    "        \n",
    "    '''\n",
    "\n",
    "    model_state = None\n",
    "    hi_accuracy = 0\n",
    "    train_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    train_acc_arr = []\n",
    "    validation_acc_arr = []\n",
    "\n",
    "\n",
    "\n",
    "    train_loader, validation_loader, test_loader = create_data_loaders(data, batch_size,train_mirror) # Initialize the dataloaders from dataset\n",
    "\n",
    "    model = model.to(device) # Initialize linear model with layer sizes\n",
    "    criterion = nn.BCELoss() # Initialize loss function - Binary Cross Entropy Loss\n",
    "    optimizer = optimizer # Initialize optimizer with starting learning rate\n",
    "\n",
    "    # Initialize result collecting lists for later plotting\n",
    "    train_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    train_acc_arr = []\n",
    "    validation_acc_arr = []\n",
    "\n",
    "    # Train the model\n",
    "    if(early_stopping):\n",
    "        early_stopper = EarlyStopper(patience=es_patience, min_delta=es_delta) # Enable early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = train(model,train_loader, optimizer, criterion=criterion, device=device, mode='Train', epoch=epoch)\n",
    "        valid_loss, valid_accuracy = test(model, validation_loader, criterion=criterion, device=device, mode='Validation', epoch=epoch)\n",
    "\n",
    "        \n",
    "        if(early_stopping):\n",
    "            if early_stopper.early_stop(valid_loss):\n",
    "                break\n",
    "\n",
    "        train_loss_arr.append(train_loss)\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "\n",
    "        try:\n",
    "            if valid_accuracy>=hi_accuracy:\n",
    "                hi_accuracy = valid_accuracy\n",
    "                print(f'New best state saved with valid. acc. = {hi_accuracy}.')\n",
    "                model_state = model.state_dict()\n",
    "        except:\n",
    "            print(\"Cannot yet save model state dict.\")\n",
    "\n",
    "        validation_loss_arr.append(valid_loss)\n",
    "        validation_acc_arr.append(valid_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plot_loss(train_loss=train_loss_arr, validation_loss=validation_loss_arr, title='ANN model loss')\n",
    "    plot_acc(train_acc_arr, validation_acc_arr, title='ANN model accuracy')\n",
    "\n",
    "    model.load_state_dict(model_state)\n",
    "    test(model, test_loader, criterion=criterion, device=device, mode='Test', epoch=epoch)\n",
    "\n",
    "    return model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loader_cv(data, batch_size, train_mirror=True, standarize=True):\n",
    "\n",
    "    '''\n",
    "    Function used to create the dataloader object for the cross-validation process\n",
    "    '''\n",
    "    # Scale the data?\n",
    "    if standarize:  \n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_ = scaler.fit_transform(X)\n",
    "\n",
    "        data_ = pd.DataFrame(X_)\n",
    "        data_['winner'] = y\n",
    "\n",
    "        data = data_\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    # train_data, test_data = train_test_split(data, test_size=0.15)\n",
    "    # train_data, validation_data = train_test_split(train_data, test_size=0.15/0.85)\n",
    " \n",
    "    # Data augmentation flip?\n",
    "\n",
    "    if train_mirror:\n",
    "        \n",
    "        X, y = train_data_mirror(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "\n",
    "    # Convert data to PyTorch tensors and create data loaders\n",
    "    if train_mirror:\n",
    "        data_dataset = TensorDataset(torch.from_numpy(X.values.astype(np.float32)), torch.from_numpy(y.values.reshape(-1, 1).astype(np.float32)))\n",
    "        data_loader = DataLoader(data_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    else:\n",
    "        data_dataset = TensorDataset(torch.from_numpy(data.iloc[:,:-1].values.astype(np.float32)), torch.from_numpy(data.iloc[:, -1].values.reshape(-1, 1).astype(np.float32)))\n",
    "        data_loader = DataLoader(data_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    print(f'Data loaded succesfully!')\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_ANN_model(train_loader, test_loader, model, criterion, optimizer, num_epochs, device='cuda', early_stopping = False, es_patience = 5, es_delta = 0.01, plot_history = True, print_history = True):\n",
    "    \n",
    "\n",
    "    '''Function used to perform a single fold validation in the cross-validation process for the ANN model'''\n",
    "\n",
    "    model_state = None\n",
    "    hi_accuracy = 0\n",
    "    train_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    train_acc_arr = []\n",
    "    validation_acc_arr = []\n",
    "\n",
    "    avg_inference_time=0\n",
    "\n",
    "    # train_loader, validation_loader, test_loader = create_data_loaders(data, batch_size,train_mirror) # Initialize the dataloaders from dataset\n",
    "\n",
    "    model = model.to(device) # Initialize linear model with layer sizes\n",
    "    # criterion = nn.BCELoss() # Initialize loss function - Binary Cross Entropy Loss\n",
    "    optimizer = optimizer # Initialize optimizer with starting learning rate\n",
    "    try:\n",
    "        criterion = criterion()\n",
    "    except:\n",
    "        print('focal loss')\n",
    "\n",
    "    # Initialize result collecting lists for later plotting\n",
    "    train_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    train_acc_arr = []\n",
    "    validation_acc_arr = []\n",
    "\n",
    "    vg_inference_time = 0\n",
    "\n",
    "    # Train the model\n",
    "    if(early_stopping):\n",
    "        early_stopper = EarlyStopper(patience=es_patience, min_delta=es_delta) # Enable early stopping\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss, train_accuracy = train(model,train_loader, optimizer, criterion=criterion, device=device, mode='Train', epoch=epoch, print_history=print_history)\n",
    "        valid_loss, valid_accuracy, inference_time = test(model, test_loader, criterion=criterion, device=device, mode='Validation', epoch=epoch, print_history=print_history)\n",
    "        avg_inference_time += inference_time\n",
    "        \n",
    "        if(early_stopping):\n",
    "            if early_stopper.early_stop(valid_loss):\n",
    "                break\n",
    "\n",
    "        train_loss_arr.append(train_loss)\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "\n",
    "        try:\n",
    "            if valid_accuracy>=hi_accuracy:\n",
    "                hi_accuracy = valid_accuracy\n",
    "                print(f'New best state saved with valid. acc. = {hi_accuracy} for epoch {epoch+1}.')\n",
    "                print(f'Average inference time for a single validation batch: {inference_time}s.')\n",
    "                model_state = model.state_dict()\n",
    "        except:\n",
    "            print(\"Cannot yet save model state dict.\")\n",
    "\n",
    "        validation_loss_arr.append(valid_loss)\n",
    "        validation_acc_arr.append(valid_accuracy)\n",
    "\n",
    "\n",
    "    if plot_history:\n",
    "        plot_loss(train_loss=train_loss_arr, validation_loss=validation_loss_arr, title='ANN model loss')\n",
    "        plot_acc(train_acc_arr, validation_acc_arr, title='ANN model accuracy')\n",
    "\n",
    "    # model.load_state_dict(model_state)\n",
    "    # test(model, test_loader, criterion=criterion, device=device, mode='Test', epoch=epoch)\n",
    "\n",
    "    return hi_accuracy, avg_inference_time/num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_split_testing_ANN(data, model_class, criterion, num_of_splits, input_size, activation_f, plot_history = True, print_history = True, early_stopping = True, optimizer = None, lr = 0.001):\n",
    "\n",
    "    '''Function used to perform the cross-validation process. The main function of this notebook.\n",
    "    Input:\n",
    "        data - dataframe used for prediction\n",
    "        model_class - pytorch class of the selected model, used to create a new object inside this function\n",
    "        criterion - pytorch loss function\n",
    "        num_of_splits - number of splits to perform in the cross-validation process\n",
    "        input_size - int, size of the input feature vector (length)\n",
    "        activation_f - pytorch acrivation function to use between the model layers\n",
    "\n",
    "        plot_history - bool, create a plot of loss and accuracy over all training epoch for training and validation\n",
    "        print_history - bool, print out the values of loss and accuracy over all training epoch for training and validation\n",
    "        early_stopping - bool, use early stopping regularization\n",
    "        optimizer - optimization algorithm, if None then Adam\n",
    "        lr - learning rate, default 0.001\n",
    "    '''\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    splits = KFold(num_of_splits)\n",
    "\n",
    "    cv_accuracy = []\n",
    "    cv_inference_time = []\n",
    "    for i, (train_split, test_split) in enumerate(splits.split(data)):\n",
    "        print(f'Training and testing {i+1}/{num_of_splits} fold...')\n",
    "\n",
    "        train_fold = data.loc[train_split,:]\n",
    "        test_fold = data.loc[test_split,:]\n",
    "\n",
    "        train_loader = create_loader_cv(train_fold, 64, False, True)\n",
    "        test_loader = create_loader_cv(test_fold, 64, False, True)\n",
    "        model = model_class(input_size=input_size, hidden_size=512, output_size=1, activation_f=activation_f)\n",
    "        if optimizer is None:\n",
    "            # Default optimizer \n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            try:\n",
    "                optimizer = optim.RAdam(model.parameters(), lr=lr)\n",
    "            except:\n",
    "                print(\"Optimizer already declared\")\n",
    "        \n",
    "        fold_test_accuracy, inference_time = cv_ANN_model(train_loader, test_loader, model, criterion, optimizer, num_epochs=50, early_stopping=early_stopping, es_patience=5, es_delta=0.002, plot_history=plot_history, print_history=print_history)\n",
    "\n",
    "        print(f'Highest validation accuracy in fold {i+1}/{num_of_splits}: {fold_test_accuracy}')\n",
    "\n",
    "        try:\n",
    "            print(f'Avg. Inference time (a single forward pass): {model.inference_time/model.n_of_forward_passes}')\n",
    "        except:\n",
    "            print('unknown inference time')\n",
    "\n",
    "        cv_accuracy.append(fold_test_accuracy)\n",
    "        cv_inference_time.append(inference_time)\n",
    "\n",
    "    print(f'Average accuracy over {num_of_splits} folds: {np.mean(cv_accuracy)}')\n",
    "    print(f'Average inference time over {num_of_splits} folds for a single validation batch: {np.mean(cv_inference_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Defining the FocalLoss function that in not natively implemented in pytorch\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='none'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        BCE_loss = F.binary_cross_entropy(input, target, reduction='mean')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used ANN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline model used for the majority of predictions\n",
    "class ANNModel_simple(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation_f):\n",
    "        super(ANNModel_simple, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation_f = activation_f\n",
    "        self.fc2 = nn.Linear(hidden_size, 128)\n",
    "        self.fc3 = nn.Linear(128,256)\n",
    "        self.fc4 = nn.Linear(256, output_size)\n",
    "        self.do1 = nn.Dropout(0.5)\n",
    "        self.do2 = nn.Dropout(0.5)\n",
    "        self.do3 = nn.Dropout(0.5)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.do1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.do2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.do3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shallow version of the baseline model\n",
    "class ANNModel_simple_shallow(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation_f):\n",
    "        super(ANNModel_simple_shallow, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation_f = activation_f\n",
    "        self.fc2 = nn.Linear(hidden_size, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "        self.do1 = nn.Dropout(0.5)\n",
    "        self.do2 = nn.Dropout(0.5)\n",
    "        self.do3 = nn.Dropout(0.5)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.do1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.do2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# The deep version of the baseline model\n",
    "class ANNModel_simple_deep(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation_f):\n",
    "        super(ANNModel_simple_deep, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.activation_f = activation_f\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128,256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128,32)\n",
    "        self.fc6 = nn.Linear(32,output_size)\n",
    "        self.do1 = nn.Dropout(0.5)\n",
    "        self.do2 = nn.Dropout(0.5)\n",
    "        self.do3 = nn.Dropout(0.5)\n",
    "        self.do4 = nn.Dropout(0.5)\n",
    "        self.do5 = nn.Dropout(0.5)\n",
    "        self.do6 = nn.Dropout(0.5)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.bn6 = nn.BatchNorm1d(output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.do1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.do2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.do3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.do4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.activation_f(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage of the above structures for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# Load data from a CSV file\n",
    "data = pd.read_csv('Teams_statistics.csv')\n",
    "data.drop(['match_id','map','team_1','team_2'],axis=1, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "input_size = data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model, loss, activation and optimizer (None = Adam)\n",
    "model_class = ANNModel_simple\n",
    "criterion = nn.BCELoss()\n",
    "activation_f = nn.ReLU()\n",
    "optimizer = None\n",
    "cv_split_testing_ANN(data, model_class, num_of_splits=5, input_size=input_size,criterion=criterion, activation_f=activation_f, plot_history=False, print_history=False, early_stopping=False, optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
